{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    ""
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "1. PyTorch 전체 학습 흐름 요약표\n",
    "| **단계**         | **역할 / 목적**                      | **사용 함수 / 클래스**                                 | **자세한 설명**                                  |\n",
    "| -------------- | -------------------------------- | ----------------------------------------------- | ------------------------------------------- |\n",
    "| ① **데이터 준비**   | 숫자 데이터를 PyTorch가 이해할 수 있게 변환     | `torch.tensor()`                                | 입력(x), 정답(y)을 tensor로 만들어야 모델이 계산할 수 있어요.   |\n",
    "| ② **모델 정의**    | 입력을 받아 예측값을 계산하는 계산기 만들기         | `nn.Module` (클래스 방식), `nn.Sequential()`         | 계산 구조를 정의하는 단계예요. 층을 쌓아 예측을 만듭니다.           |\n",
    "| ③ **손실 함수 정의** | 예측값과 실제값의 차이를 계산                 | `nn.MSELoss()`, `nn.CrossEntropyLoss()` 등       | 얼마나 틀렸는지를 숫자로 계산. 회귀는 MSE, 분류는 CrossEntropy |\n",
    "| ④ **옵티마이저 설정** | 오차를 줄이기 위해 **가중치를 어떻게 고칠지** 결정   | `optim.SGD()`, `optim.Adam()` 등                 | 손실을 줄이기 위해 가중치(W)와 절편(b)을 조정하는 방식 지정        |\n",
    "| ⑤ **학습 루프**    | 모델이 데이터를 보고 예측하고, 틀린 걸 고치며 학습 반복 | `.zero_grad()`, `.backward()`, `.step()`        | 예측 → 손실 → 역전파 → 가중치 업데이트 과정을 반복 수행          |\n",
    "| ⑥ **예측 및 평가**  | 학습된 모델로 새 데이터를 예측                | `model(x)` / `with torch.no_grad()` / `.eval()` | 예측만 할 땐 `no_grad()`와 `eval()`로 불필요한 계산을 막아요 |\n"
   ],
   "id": "235e8ec2fe1b6290"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "2. PyTorch 기본 구조\n",
    "| **요소**          | **자세한 설명**                                                                                                                                         |\n",
    "| --------------- | -------------------------------------------------------------------------------------------------------------------------------------------------- |\n",
    "| `tensor`        | 딥러닝에서 다루는 모든 숫자 데이터를 담는 기본 구조입니다. 수학으로 말하면 행렬이나 배열이에요. 입력 데이터(x), 정답 데이터(y), 가중치(W) 모두 이 tensor라는 형태로 표현돼요. **예시**: `torch.tensor([[1.0], [2.0]])` |\n",
    "| `nn.Module`     | PyTorch에서 모든 모델(계산기 구조)은 `nn.Module`을 **기반으로 만든 클래스**예요. 직접 클래스를 만들고 그 안에 어떤 레이어를 어떻게 연결할지 적을 수 있어요. **모델의 뼈대 역할**을 해요.                            |\n",
    "| `nn.Linear`     | 입력값에 **가중치를 곱하고 편향을 더해서** 결과를 계산하는 레이어입니다. 수식으로는 `y = W*x + b`입니다. 입력과 출력 크기를 정해서 쓰며, 모델 안에서 여러 번 사용할 수 있어요.                                       |\n",
    "| `nn.Sequential` | 여러 레이어를 **순서대로 간단히 연결**할 수 있게 도와주는 도구입니다. 모델 구조가 단순할 때 매우 편리하며, `forward()` 함수를 따로 쓰지 않아도 됩니다. 복잡한 모델은 `nn.Module`을 써야 합니다.                        |\n",
    "| `to(device)`    | 모델이나 데이터를 **GPU로 보내는 명령어**입니다. GPU는 계산이 빠르기 때문에 딥러닝에서 자주 사용돼요. `cuda`는 GPU를 의미하며, CPU와 GPU가 다르면 에러가 나기 때문에 반드시 `.to()`를 맞춰줘야 해요.                   |\n",
    "| `DataLoader`    | 데이터를 한 번에 조금씩 나눠서 모델에 전달할 수 있도록 도와주는 도구입니다. 이렇게 나눈 걸 \\*\\*미니배치(mini-batch)\\*\\*라고 해요. 큰 데이터를 한꺼번에 처리하면 메모리 부족이 날 수 있기 때문에 꼭 필요해요.                    |\n",
    "| `loss function` | 모델의 예측값이 정답과 **얼마나 차이가 나는지 수치로 알려주는 함수**예요. 이 값을 기준으로 모델을 고쳐요. 회귀문제는 `MSELoss`, 분류문제는 `CrossEntropyLoss`를 주로 사용해요.                                 |\n",
    "| `optimizer`     | 모델이 틀렸을 때 **가중치와 절편을 어떻게 수정할지** 알려주는 알고리즘입니다. `SGD`는 기본적인 방식이고, `Adam`은 더 똑똑하게 수정해요. 학습 속도(learning rate)도 같이 설정해줘요.                               |\n",
    "\n"
   ],
   "id": "f23517325aecb869"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "3. PyTorch 핵심 함수/메서드\n",
    "| **함수 / 메서드**           | **자세한 설명**                                                                               | **예시 코드**                               |\n",
    "| ---------------------- | ---------------------------------------------------------------------------------------- | --------------------------------------- |\n",
    "| `torch.tensor()`       | 리스트나 숫자를 \\*\\*tensor(텐서)\\*\\*라는 PyTorch의 데이터 구조로 변환합니다. 딥러닝에서 입력, 정답, 계산 모두 tensor로 표현합니다. | `x = torch.tensor([[1.0], [2.0]])`      |\n",
    "| `.shape` / `.size()`   | 텐서의 \\*\\*모양(차원, 크기)\\*\\*를 확인할 때 사용합니다. 학습 전에 입력 형식이 맞는지 확인하는 데 필수입니다.                      | `x.shape`, `x.size()`                   |\n",
    "| `.view()`              | 텐서의 **형태(차원 수)를 바꿀 때** 사용합니다. reshape과 비슷하며, 데이터 개수는 그대로 유지됩니다.                          | `x.view(4)`                             |\n",
    "| `.item()`              | 하나의 숫자만 담긴 텐서에서 **숫자 값만 꺼낼 때** 사용합니다. 출력값이나 loss 값 등을 숫자처럼 출력하고 싶을 때 유용합니다.              | `loss.item()`                           |\n",
    "| `.detach()`            | 텐서를 연산 기록에서 **분리**해서 **gradient 계산에서 제외**할 때 사용합니다. 예측 결과를 복사하거나 출력할 때 주로 사용합니다.         | `output = model(x).detach()`            |\n",
    "| `.requires_grad_()`    | 텐서에 대해 **미분(gradient) 계산을 할지 말지 설정**합니다. 학습할 파라미터는 True, 아닌 경우 False로 설정합니다.             | `x.requires_grad_()`                    |\n",
    "| `with torch.no_grad()` | **gradient를 계산하지 않는 상태**로 전환합니다. 평가나 테스트처럼 학습이 필요 없는 상황에서 연산량을 줄이기 위해 사용합니다.             | `with torch.no_grad(): pred = model(x)` |\n",
    "| `.train()`             | 모델을 **학습 모드**로 전환합니다. Dropout, BatchNorm 등의 레이어가 학습용으로 동작하게 만듭니다.                        | `model.train()`                         |\n",
    "| `.eval()`              | 모델을 **평가 모드**로 전환합니다. Dropout이나 BatchNorm 등의 레이어가 예측용으로 동작하게 만듭니다.                       | `model.eval()`                          |\n",
    "\n"
   ],
   "id": "49d709fd6e37bec8"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "4. 딥러닝 활성화 함수 정리표\n",
    "| **이름**        | **출력 범위**   | **수식 특징 & 동작 원리**                                                           | **장점 / 단점**                                                                          | **주 사용 위치**      |\n",
    "| ------------- | ----------- | --------------------------------------------------------------------------- | ------------------------------------------------------------------------------------ | ---------------- |\n",
    "| **Sigmoid**   | (0, 1)      | 입력값을 S자 곡선 형태로 압축. 큰 값 → 1, 작은 값 → 0<br>출력값은 확률처럼 해석 가능                     | ✔ 확률처럼 해석 가능<br>✖ 출력이 0 또는 1에 가까우면 gradient(기울기)가 0 → 학습 느림 (**vanishing gradient**) | **출력층** (이진 분류)  |\n",
    "| **Tanh**      | (-1, 1)     | Sigmoid와 비슷하지만 중심이 0<br>양수는 양수, 음수는 음수로 부호 유지                               | ✔ 중심이 0이라 학습이 더 안정적<br>✖ 큰 입력값에선 gradient가 0으로 수렴할 수 있음                              | **중간층** (은닉층)    |\n",
    "| **ReLU**      | \\[0, ∞)     | 0보다 작으면 0, 크면 그대로 출력<br>수식: `f(x) = max(0, x)`                              | ✔ 계산 간단하고 빠름<br>✔ gradient가 유지되어 깊은 신경망에서도 잘 작동<br>✖ 음수는 0으로 죽음 (**dead ReLU**)      | **대부분의 은닉층**     |\n",
    "| **LeakyReLU** | (-∞, ∞)     | ReLU의 문제 해결: 음수일 때도 `0.01x`처럼 아주 작게 출력<br>수식: `f(x) = x (x>0), 0.01x (x≤0)` | ✔ ReLU의 죽는 문제 해결<br>✖ 하이퍼파라미터(0.01)를 조절해야 함                                          | **ReLU 대체 필요 시** |\n",
    "| **Softmax**   | (0\\~1, 합=1) | 여러 출력값을 **전체 합이 1인 확률 분포**로 변환<br>예: `[2.0, 1.0, 0.1]` → `[0.7, 0.2, 0.1]`  | ✔ 다중 클래스 분류에 필수<br>✔ 확률처럼 해석 가능<br>✖ 수치 불안정 시 로그와 함께 사용 필요 (`log-softmax`)           | **출력층** (다중 분류)  |\n",
    "\n"
   ],
   "id": "dd2fb7e76983a83f"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "4-1 활성화 함수 선택 가이드\n",
    "| 목적 / 상황            | 추천 함수       | 이유                      |\n",
    "| ------------------ | ----------- | ----------------------- |\n",
    "| **이진 분류 (0 또는 1)** | `Sigmoid`   | 0\\~1 사이 확률 출력           |\n",
    "| **다중 클래스 분류**      | `Softmax`   | 여러 클래스 중 하나 선택 (확률로 출력) |\n",
    "| **은닉층 (기본값)**      | `ReLU`      | 계산 간단, 학습 빠름            |\n",
    "| **ReLU 뉴런이 죽을 때**  | `LeakyReLU` | 음수도 작게 통과시켜 뉴런 죽음 방지    |\n",
    "| **중심이 0인 게 유리할 때** | `Tanh`      | 양/음 부호 유지, 학습 안정화에 도움   |\n",
    "\n"
   ],
   "id": "b01ef093e6ba650d"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "5. 손실 함수 정리표\n",
    "| **손실 함수**               | **설명**                                                                          | **주 용도**  | **출력층 활성화 함수**       |       |    |\n",
    "| ----------------------- | ------------------------------------------------------------------------------- | --------- | -------------------- | ----- | -- |\n",
    "| `nn.MSELoss()`          | 예측값과 실제값의 **차이를 제곱해서 평균** 낸 값. 차이가 클수록 손실이 더 커짐 (민감함).<br>예: `(ŷ - y)²`        | 회귀 문제     | 없음 (그냥 숫자 출력)        |       |    |\n",
    "| `nn.L1Loss()`           | 예측값과 실제값의 **절댓값 차이의 평균**.<br>예: \\`                                              | ŷ - y    | \\` → 이상치에 덜 민감함      | 회귀 문제 | 없음 |\n",
    "| `nn.CrossEntropyLoss()` | **소프트맥스 + 로그우도 오차**를 합친 함수.<br>예측이 정답과 다를수록 손실이 커짐.<br>클래스 분류에 최적화              | 다중 클래스 분류 | 없음 (`Softmax` 자동 내장) |       |    |\n",
    "| `nn.BCELoss()`          | 예측값이 확률(0\\~1)일 때 **이진 분류용 교차 엔트로피 오차** 계산.<br>예: `-y·log(ŷ) - (1-y)·log(1-ŷ)` | 이진 분류     | `Sigmoid()` 필요함      |       |    |\n"
   ],
   "id": "3d9ffde3a0f3b44e"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "5-1.언제 어떤 손실 함수를 써야 하나\n",
    "| 문제 유형     | 출력 값 형태          | 출력층 활성화 함수      | 추천 손실 함수                   |\n",
    "| --------- | ---------------- | --------------- | -------------------------- |\n",
    "| 회귀        | 실수 값 (예: 3.5)    | 없음              | `nn.MSELoss()`, `L1Loss()` |\n",
    "| 이진 분류     | 0 또는 1 확률값       | `Sigmoid`       | `nn.BCELoss()`             |\n",
    "| 다중 클래스 분류 | 클래스 번호 (예: 0\\~2) | 없음 (Softmax 내장) | `nn.CrossEntropyLoss()`    |\n"
   ],
   "id": "8e9b14d38f443f22"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "6. 최적화 함수 정리\n",
    "\\| **Optimizer** | **자세한 설명**                                                                                                                  | **특징 요약**                    | **추천 사용 상황**      |\n",
    "| ------------- | --------------------------------------------------------------------------------------------------------------------------- | ---------------------------- | ----------------- |\n",
    "| **SGD**       | Stochastic Gradient Descent (확률적 경사 하강법)<br>가장 기본적인 방식. 한 번에 미니배치만 보고 기울기를 계산하여 가중치를 업데이트함.                                 | ✔ 단순하고 빠름<br>✖ 튜닝이 필요함       | 소규모 모델, 기본 학습 구조  |\n",
    "| **Adam**      | Adaptive Moment Estimation.<br>SGD에 \\*\\*기울기의 평균(m)\\*\\*과 \\*\\*분산(v)\\*\\*을 자동으로 추적하며 학습률을 알아서 조정함.<br>**가장 많이 쓰이는 옵티마이저** 중 하나. | ✔ 학습률 자동 조절<br>✔ 거의 항상 잘 작동  | 초보자 기본 선택, 복잡한 모델 |\n",
    "| **RMSprop**   | **최근 기울기의 제곱 평균을 기준으로 학습률을 줄임.**<br>진동을 줄이고 학습을 더 안정적으로 유지. RNN에서 자주 쓰임.                                                    | ✔ 진동 억제<br>✔ 빠른 수렴           | RNN, 시계열 데이터      |\n",
    "| **Adagrad**   | 과거의 모든 기울기 정보를 누적하여 **자주 등장하는 파라미터는 학습률을 줄이고, 드문 파라미터는 높임.**<br>희소한(드물게 등장하는) 데이터에 잘 작동함.                                   | ✔ 희소 데이터에 강함<br>✖ 학습률 빨리 줄어듬 | 희소 특성(예: 단어, 텍스트) |\n"
   ],
   "id": "5ef9a1a52b8e1a0b"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "7.  기타 기능들\n",
    "| **기능**                   | **정확한 설명**                                                               | **사용 목적**              | **사용 위치 / 예시**                                                 |\n",
    "| ------------------------ | ------------------------------------------------------------------------ | ---------------------- | -------------------------------------------------------------- |\n",
    "| **Dropout**              | 학습 중 무작위로 일부 뉴런을 끄는 기법.<br>**과적합(overfitting)** 방지에 효과적. 테스트 때는 전체 사용.   | ✔ 과적합 방지               | 은닉층 중간에 사용<br>`nn.Dropout(p=0.5)`                              |\n",
    "| **BatchNorm**            | 각 레이어의 출력값을 평균 0, 분산 1로 정규화해서 **학습을 안정화**함.<br>속도 향상 및 초기화 민감도 감소.       | ✔ 학습 안정화<br>✔ 수렴 가속    | 레이어 뒤에 사용<br>`nn.BatchNorm1d()`                                |\n",
    "| **EarlyStopping**        | **검증 성능이 더 이상 좋아지지 않으면 학습을 중단**함. 불필요한 에폭을 방지. 일반적으로 val\\_loss 기준으로 판단.  | ✔ 과적합 방지<br>✔ 훈련 시간 절약 | PyTorch에서는 직접 구현 or 콜백 사용                                      |\n",
    "| **Gradient Clipping**    | 역전파 시 \\*\\*기울기(gradient)가 너무 커지는 것(폭주)\\*\\*을 방지함.<br>폭주하면 학습이 발산함.         | ✔ 안정된 학습 유지            | `torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm)` |\n",
    "| **Scheduler (러닝레이트 조정)** | 학습률을 점점 줄이면서 **더 세밀하게 최적값에 접근**하게 함. 일반적으로 에폭 수에 따라 또는 val\\_loss에 따라 줄임. | ✔ 과적합 방지<br>✔ 성능 향상    | `StepLR`, `ReduceLROnPlateau` 등 사용                             |\n"
   ],
   "id": "f825f5876e860220"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "",
   "id": "e3c6c1054db9e842"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
